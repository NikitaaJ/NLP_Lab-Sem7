{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nX2ZD-xcjXB6",
    "outputId": "34788966-79cd-4139-a14e-a4a4541b8c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: python, Stemmed: python\n",
      "Original: pythoner, Stemmed: pythoner\n",
      "Original: pythoning, Stemmed: pythoning\n",
      "Original: pythoned, Stemmed: pythoned\n",
      "Original: pythonly, Stemmed: pythonly\n"
     ]
    }
   ],
   "source": [
    "def porter_stemmer(word):\n",
    "    \"\"\"\n",
    "    Apply the Porter Stemmer algorithm to a given word.\n",
    "    \"\"\"\n",
    "    # Define the list of rules\n",
    "    rules = [\n",
    "        (u\"'s'\", u\"\"),\n",
    "        (u\"sses\", u\"ss\"),\n",
    "        (u\"ies\", u\"i\"),\n",
    "        (u\"ss\", u\"ss\"),\n",
    "        (u\"s\", u\"\")\n",
    "    ]\n",
    "\n",
    "    # Apply each rule\n",
    "    for rule in rules:\n",
    "        suffix, replacement = rule\n",
    "        if word.endswith(suffix):\n",
    "            if len(word) > len(suffix):\n",
    "                return word[:-len(suffix)] + replacement\n",
    "            else:\n",
    "                return replacement\n",
    "\n",
    "    return word\n",
    "words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "# Stem each word and print the result\n",
    "for word in words:\n",
    "    stemmed_word = porter_stemmer(word)\n",
    "    print(f\"Original: {word}, Stemmed: {stemmed_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caZ2lJOhjd8J",
    "outputId": "d5c9ca25-5810-4990-82d2-a52a05322504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "example_words =[\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "  print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Frv7k6v7kJuo",
    "outputId": "f3143f2c-e3c2-43b7-fffe-547461808200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running, Stemmed: runn\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def porter_stemmer(word):\n",
    "    \"\"\"\n",
    "    Apply the Porter stemming algorithm to a word.\n",
    "    \"\"\"\n",
    "    # Step 1a\n",
    "    step_1a_suffixes = ['sses', 'ies', 'ss', 's']\n",
    "    for suffix in step_1a_suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            word = word[:-len(suffix)]\n",
    "            break\n",
    "\n",
    "    # Step 1b\n",
    "    step_1b_suffixes = ['eed', 'ed', 'ing']\n",
    "    for suffix in step_1b_suffixes:\n",
    "        if re.search(suffix, word):\n",
    "            if re.search(r'[aeiouy]', word[:-len(suffix)]):\n",
    "                word = word[:-len(suffix)]\n",
    "                if suffix == 'eed' and re.search(r'ed', word):\n",
    "                    word = word[:-1]\n",
    "                break\n",
    "\n",
    "    # Step 1c\n",
    "    if re.search(r'y', word):\n",
    "        if re.search(r'[aeiouy]', word[:-1]):\n",
    "            word = word[:-1] + 'i'\n",
    "\n",
    "    # Step 2\n",
    "    step_2_suffixes = [\n",
    "        ('ational', 'ate'), ('tional', 'tion'), ('enci', 'ence'),\n",
    "        ('anci', 'ance'), ('izer', 'ize'), ('bli', 'ble'),\n",
    "        ('alli', 'al'), ('entli', 'ent'), ('eli', 'e'),\n",
    "        ('ousli', 'ous'), ('ization', 'ize'), ('ation', 'ate'),\n",
    "        ('ator', 'ate'), ('alism', 'al'), ('iveness', 'ive'),\n",
    "        ('fulness', 'ful'), ('ousness', 'ous'), ('aliti', 'al'),\n",
    "        ('iviti', 'ive'), ('biliti', 'ble')\n",
    "    ]\n",
    "    for suffix, replacement in step_2_suffixes:\n",
    "        if re.search(suffix, word):\n",
    "            if len(word) - len(suffix) > 1:\n",
    "                word = word[:-len(suffix)] + replacement\n",
    "\n",
    "    # Step 3\n",
    "    step_3_suffixes = [\n",
    "        ('icate', 'ic'), ('ative', ''), ('alize', 'al'),\n",
    "        ('iciti', 'ic'), ('ical', ''), ('ful', ''),\n",
    "        ('ness', '')\n",
    "    ]\n",
    "    for suffix, replacement in step_3_suffixes:\n",
    "        if re.search(suffix, word):\n",
    "            if len(word) - len(suffix) > 1:\n",
    "                word = word[:-len(suffix)] + replacement\n",
    "\n",
    "    # Step 4\n",
    "    if re.search(r'al$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-2]\n",
    "    elif re.search(r'ance$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-4]\n",
    "    elif re.search(r'ence$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-4]\n",
    "    elif re.search(r'able$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-4]\n",
    "    elif re.search(r'ible$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-4]\n",
    "    elif re.search(r'ant$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ement$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-6]\n",
    "    elif re.search(r'ment$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-4]\n",
    "    elif re.search(r'ent$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ion$', word):\n",
    "        if len(word) > 5 and (word[-4] == 's' or word[-4] == 't'):\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ou$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-2]\n",
    "    elif re.search(r'ism$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ate$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'iti$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ous$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ive$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "    elif re.search(r'ize$', word):\n",
    "        if len(word) > 5:\n",
    "            word = word[:-3]\n",
    "\n",
    "    # Step 5a\n",
    "    if re.search(r'e$', word):\n",
    "        if len(word) - 1 >= 3:\n",
    "            if re.search(r'[aeiouy]', word[:-1]):\n",
    "                word = word[:-1]\n",
    "\n",
    "    # Step 5b\n",
    "    if re.search(r'lli$', word):\n",
    "        if len(word) - 1 >= 3:\n",
    "            if re.search(r'[aeiouy]', word[:-1]):\n",
    "                word = word[:-1] + 'y'\n",
    "\n",
    "    return word\n",
    "\n",
    "# Example usage\n",
    "word = \"running\"\n",
    "stemmed_word = porter_stemmer(word)\n",
    "print(f\"Original: {word}, Stemmed: {stemmed_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJvFEfjTkOYT",
    "outputId": "682a91ea-c651-4060-a53c-406f8e854956"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mannp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ny5AZ-zokP2a",
    "outputId": "f7a9c262-ee3a-4b71-c5e5-2295792af973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am run in the park and eat appl\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"I am running in the park and eating apples\"\n",
    "stemmed_text = porter_stemmer(input_text)\n",
    "print(stemmed_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
